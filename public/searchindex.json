[{"content":"I have 50+ tabs related to this topic and the worst writer\u0026rsquo;s block in years. So instead of writing a full-blown review of the current community efforts to generate images, I\u0026rsquo;ll give a short overview of my thoughts.\nTo get a basic idea of what I am talking about, look at https://softologyblog.wordpress.com/2021/11/25/text-to-image-summary-part-5/ https://twitter.com/RiversHaveWings\nIn the last 9 months, there has been a lot of enthusiasm in the NN community about image generation, specifically, text guided image generation. At his point, this enthusiasm is very much misplaced. We can generate cool images, we can generate psychedelic videos, we can even sell them as NFTs. But practically, the results are all but useless. And I am not talking about 1 specific type of image generation. You can use VAE, GAN, FLOW, Diffusion, Score and you all will get is a set of very cool wallpapers. Maybe I am missing the point. Maybe the point of art is to get an emotional response, and AI Art does do that. But from a content creation standpoint, there needs to be a different overarching objective than the one we currently see in scientific papers.\nLet\u0026rsquo;s say I want to tell a story. It can be a book, a game, a video, a clip, an interactive novel or something else entirely. But I want the story to be accompanied by images. How do I automate that?\nFirst, I need to enumerate all the images am going to need. (Create a storyboard)\nThen I need to choose a common style for all of them. (Choose a style (what is style?))\nAnd finally, produce the images. (Generate images)\nA wrong image generation pipeline Let\u0026rsquo;s talk automatic image generation. This is the task that the scientific community is trying to solve right now. Here is the established pipeline (if we ignore model training and finetuning):\n Choose a base image Choose a generator Choose an optimizer Choose losses Create a prompt Iterate Get a result  There are two interconnected problems with this approach.\n I have little agency in the result. It does not work.  Oh, it does produce cool images. But there is a snowball\u0026rsquo;s chance in hell you are getting a storybook to illustrate anything other than an acid trip. You can get good images with a high amount of detail. They are just not going to be what you want to illustrate or how you want to illustrate, no matter how much you change the prompt or adjust the losses. Again do not think 1 cool image, think a coherent story written by you but told by generated images. And even if you could do that. Is that as a content creator what you really want? The visuals defined not by you but by the model? Random details you have no control over?\nWhat I believe to be a better approach is to leave more agency with the human creator and give him a more comprehensive set of tool, a chance to guide the generation, by part of the process and not just as a bystander.\nImage Generation pipeline assisted by AI (or may be assisted by a human)  Get a set of base associations/inspirations (We can use the \u0026ldquo;wrong\u0026rdquo; pipeline to generate these) Choose a generator Get a rough first iterations Segment images into parts you like and don\u0026rsquo;t like Specify the edits for each part  Specify some new inspiration Specify things you do NOT want Embolden parts that are good Add additional elements to the image   Specify the losses for each part  When I imagine a picture, a character, a location, I can usually come up with a number of inspirations: a scene from the movie, a song, a description from the book, a real live photo I took. Possible inspirations:\n A text prompt A style image A content image A scribble An outline  Once there is a rough first result, I often can point to objects that should be removed or can give a rough sketch (scribble) of a detail I want to be added. Like removing Bob Ross silhouette from a picture generated by \u0026lsquo;Happy little tree drawn by Bob Ross\u0026rsquo; or adding a spaceship to a \u0026lsquo;Spaceport on top of an alien ocean\u0026rsquo;. We actually have the tools to do edits like these. It is just a matter of defining segments of the images and using one of the well-known losses.\nI am looking forward to a chance to experiment with NUWA and DiffusionClip (despite the bad reviews at https://openreview.net/forum?id=TKMJ9eqtpgP)\nThey might be a better backbone for the pipeline I\u0026rsquo;ve described above.\nSome of my experiments: \r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\rif (!jQuery) {\ralert(\"jquery is not loaded\");\r}\r$( document ).ready(() = {\rconst gallery = $(\"#gallery-7963e8f11a549eb2a7f8d76d7f82689e-0\");\rlet swipeboxInstance = null;\rgallery.on('jg.complete', () = {\r$(() = {\r$('.lazy').Lazy({\rvisibleOnly: true,\rafterLoad: element = element.css({filter: \"none\", transition: \"filter 1.0s ease-in-out\"})\r});\r});\rswipeboxInstance = $('.galleryImg').swipebox(\rjQuery.extend({},\r{ }\r)\r);\r});\rgallery.justifiedGallery({\rrowHeight : \"150\",\rmargins : \"5\",\rborder : 0,\rwaitThumbnailsLoad : false,\rlastRow : \"justify\",\rcaptions : false,\r});\r});\r\r","id":0,"section":"posts","summary":"I have 50+ tabs related to this topic and the worst writer\u0026rsquo;s block in years. So instead of writing a full-blown review of the current community efforts to generate images, I\u0026rsquo;ll give a short overview of my thoughts.\nTo get a basic idea of what I am talking about, look at https://softologyblog.wordpress.com/2021/11/25/text-to-image-summary-part-5/ https://twitter.com/RiversHaveWings\nIn the last 9 months, there has been a lot of enthusiasm in the NN community about image generation, specifically, text guided image generation.","tags":["AI tools"],"title":"Generating images with AI","uri":"https://ludentes.ru/posts/image-generation/","year":"2021"},{"content":"I have recently got a chance to colorize a dozen photos of my grandfather. I wanted to document thre results here as I think they indicate an important pattern in practical AI.\nTLDR For personal use, choose complete apps and MyHeritage in particular. For industrial one, choose solutions optimized for your problem rather than the SOTA models and DeOldify in particular. When using the newest model, be prepared to spend months ironing out the kinks.\nModels There are several open-source solutions backed by scientific papers available right now. The ones with the most buzz about them are probably DeOldify and Colorization Transformer.\nDeOldify is an old horse that has been around for more than 3 years already and all 3 years it was getting better. What is probably even more important it was optimized specifically to work with old photos and videos.\nColorization Transformer is a new cool kid on the block. One with a very impressive resume, good theoretic backing, developed by Google and beautifully documented results. Those results demonstrate that the model works for colored images which were converted to black-and-white. But is that the same as colorizing old photos?\nExperiment I\u0026rsquo;ll use just one photo as a demonstration, but I had very similar results with all 12 of them. We start with:\nThen we can use [DeOldify colab] (https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColabStable.ipynb) to produce a somewhat boring, but realistic image: Colorization Transformer gives us a resized, rescale, cropped, image with random coloring:\nAnd special improved DeOldify implementation (available at https://myheritage.com/) produces a very decently colored image: Conclusions A well-polished solution for your particular problem will generally beat (actually olbiterate) a new model. If all you have is a scientific prototype, you must understand that a road to a fully production ready system is long and bumpy. And I am not talking about deployment here. I am talking about actually finetuning the solution to your particular domain. And this goes far beyond just finetuning the model and often involves reevaluating every step of the initial solution.\nSo when you can use an available service, otherwise try to stick with a battletested (or maybe battle-optimized) solution. And if you actually want to be a trailblazer - prepare for a very bumpy ride (which in business terms means lots and lots of expenses).\n","id":1,"section":"posts","summary":"I have recently got a chance to colorize a dozen photos of my grandfather. I wanted to document thre results here as I think they indicate an important pattern in practical AI.\nTLDR For personal use, choose complete apps and MyHeritage in particular. For industrial one, choose solutions optimized for your problem rather than the SOTA models and DeOldify in particular. When using the newest model, be prepared to spend months ironing out the kinks.","tags":["Stylegan","Image generation","Data analysis"],"title":"Colorizing with old and new","uri":"https://ludentes.ru/posts/colorizing-old-photos/","year":"2021"},{"content":"    “You’d be surprised how much gray matter is dedicated to the analysis of facial imagery. Shame to waste it on anything as … counterintuitive as residual plots or contingency tables.”\nWatts, Peter. Blindsight (Firefall) (p. 337). Tom Doherty Associates. Kindle Edition.\n Before reading any further try to find a pattern in the video. Is eye size related to the hair color or maybe the older the face the wider the nose?\nThe Experiment Recently Nvidia published the Stylegan3 so I decided it is past time to close my tabs with Stylegan2. Peter Watts theorized that our brains are more accustomed to distinguishing patterns in faces than in boring and artificial graphs and he proposed an interface for view data\n “Skull diameter scales to total mass. Mandible length scales to EM transparency at one angstrom. One hundred thirteen facial dimensions, each presenting a different variable. Principle-component combinations present as multifeature aspect ratios.”\n  Watts, Peter. Blindsight (Firefall) (p. 337). Tom Doherty Associates. Kindle Edition.\n So I decided to look into that. Let\u0026rsquo;s say we have some data where Y is linearly dependent on X (Y = aX + b). First, let’s add some noise and get something like this \r\rY is linearly dependent on X. The exact values are of no consequence\r\r\rTo make the task a little bit more interesting let’s add 4 more dimensions to X. However to make the experiment a little easier to understand let\u0026rsquo;s have Y dependent only on one of X 5 dimensions.\nX, y = make_regression(\rn_samples=212, n_features=5, n_informative = 1, random_state=0, noise=8.0\r)\rNow we can visualize this data using a human face. First, let\u0026rsquo;s get a face. Our centre of coordinates. In Blindsight the interface was optimized for a vampire I went with something a little bit more pedestrian. Now let’s say we have 6 different axes (about drawbacks of this approach later) we can use to \u0026ldquo;morph\u0026rdquo; the image.\n age smile gender eye size hair black hair red  So we use Y for our smile axis, informative X as our age axis and all uninformative X values for other axes.\nNow for every point of the data, we can generate an image.\nWe can make the task of finding patterns a little bit more interesting by adding outliers - data points that do not obey the original pattern.\n\r\rY is linearly dependent on X. We also added some outliers to the data\r\r\rRepeating the previous process we get   Or alternatively, we can make the task easier and sort the data points by Y\n  Problems with the approach This little experiment can hardly be called scientific.\n The basic approach based on latent directions (smile, age, etc.) for Stylegan is flawed because the vectors (directions) are imperfect and not independent at all (it is pretty obvious that going along the age direction we also get someone with a more pronounced Asian ethnicity and more pronounced \u0026ldquo;eye asymmetry\u0026rdquo;). The mind\u0026rsquo;s ability to find patterns has to be measured somehow and this approach compared with other forms of multidimensional data representation. I am using vectors derived for Stylegan 2 against Stylegan2-ada network (here are videos for Stylegan network (https://vimeo.com/647172530, https://vimeo.com/647164228)).  But a straight comparison of data presented as a series of faces with data presented as a huge table does give some credence to the idea of \u0026ldquo;human face-based data visualization\u0026rdquo;.\nThe Prototype Regrettably, I do not have access to GPU enabled server, so for now let\u0026rsquo;s just give a rough sketch of what an app for further experimentations might look like.\n I want a web app where anyone can upload a medium-sized dataset to visualize using Stylegan. The app should allow the user to specify what columns should correspond to what latent vectors. Additionally the app should allow the user to forgo the usage of the latent vectors and project the data into the latent space directly. The user should be able to specify basic transformations for every column (scaling, log, shift). The user should be able to view the result either as a video or a series of images.  Looks straightforward enough. I hope to come back to this idea after some experiment with Stylegan 3.\n","id":2,"section":"posts","summary":"“You’d be surprised how much gray matter is dedicated to the analysis of facial imagery. Shame to waste it on anything as … counterintuitive as residual plots or contingency tables.”\nWatts, Peter. Blindsight (Firefall) (p. 337). Tom Doherty Associates. Kindle Edition.\n Before reading any further try to find a pattern in the video. Is eye size related to the hair color or maybe the older the face the wider the nose?","tags":["Stylegan","Image generation","Data analysis"],"title":"Using neurons and spotting patterns","uri":"https://ludentes.ru/posts/vamp-interface/","year":"2021"},{"content":"A beginning There is a lot of stuff on the Internet. When I find something interesting I tend to leave a tab open, as an unsolved task, as something to get back to. Way too often I never get back but the tabs remain open. Silent, judging, unread. And even when do read them often I need to experiment, to save results in order to fully process the info. Thus this blog. For some time now the majority of my unclosed tabs are about AI tools, promises of actually helpful neural networks, articles scientific and readable. Here are the topics I plan to close the tabs about\u0026hellip;\nAI writing assistants G2 grid (a square with Leader, contenders) is awfully crowded. But all these tools are probably good for something and useless for others. I generally let Grammarly check my writing so we can assume that this whole blog is a practical demo of Grammarly as a corrector. But what if\n I want my text to be rewritten as if it was written by a native speaker. I want to generate a short text (fictional description) and edit it later. I want text summarized “abstractively” (see extractive vs abstractive summarization). I want the whole writing process to be as automated as possible (I have no idea what the process is really like). I am looking for things I didn’t even think to solve with a tool.  I am going to leave most copywriter stuff outside the scope for now (write a press release, generate a daily Telegram post, etc). Maybe I’ll get back to it later.\nI am going to try to test the tools, produce results and document the results here.\nContent management and search using deep neural networks Neural networks nowadays can understand the content. Or at least fake this understanding pretty well (see Chinese room). This open possibility for a number of practical applications. Just for example, we can actually organize all our clutter in the Downloads folder or better yet or the content we sift through on a day-to-day basis.\nAnd all the building blocks are actually there:\n We can get a webpage from the chrome extension. Parse webpage and extract text, images (multimedia), metadata with trafilatura. Create a multimodal representation of the data using SBERT and other NN. Use weaviate or milvus to help us create a production-ready search application.  If I ever come up with a prototype with a system like this it will not only make my life a lot easier it will also close like 20 unread tabs in my browser (or let me close all the tabs and have the data well organized in the shiny new system)\nContent creation using neural networks NN can restyle images. NN can generate images. NN can write music. But the music sucks, the images are limited and styling looks cool only for the first time you see it. Meta and Nvidia are talking a lot about digital avatars. About a way to create a digital twin for a person. What if our twins could do better than State-of-the-art \u0026ldquo;creative\u0026rdquo; NN, or at the very least what if NN could use our twins to create content that we want or like. Or maybe at least helpful for us specifically. As such, I plan to close some tabs about creating systems like https://tavus.io/ or using StyleGan3 for something practical or at least cool https://www.youtube.com/watch?v=9QuDh3W3lOY\nGame system optimization RL is fun. Multiagent game theory is math. Solving some games is a matter of throwing enough computing power at them. But what if we do not want to just solve a game. What if we want to design a game that is\n Interesting Balanced Has a sophisticated and tunable AI available  The framework for automating game system design is actually the same as for designing the agent who can play the game. It\u0026rsquo;s just a matter of reinterpreting the parameters and throwing the computer power at it. At least in theory. What I want to know is what it is like in practice.\nGithub repo analysis A surprising number of people have in recent years asked me about the possibility of utilizing Jira, Confluence, Github data to optimize processes in teams. Honestly, I was only able to give general advice regarding text embedding and data representation. Then it occurred to me how much time I could have saved if I knew which repos have bugs, which project is about to close and which library is going to be mainstream in half a year. And for most open source projects is data is readily available. You just have to use well-known methods to analyze it.\n","id":3,"section":"posts","summary":"A beginning There is a lot of stuff on the Internet. When I find something interesting I tend to leave a tab open, as an unsolved task, as something to get back to. Way too often I never get back but the tabs remain open. Silent, judging, unread. And even when do read them often I need to experiment, to save results in order to fully process the info. Thus this blog.","tags":["blog","AI tools"],"title":"Closing the tabs","uri":"https://ludentes.ru/posts/closing-tabs/","year":"2021"},{"content":"","id":4,"section":"","summary":"archives","tags":null,"title":"Archive","uri":"https://ludentes.ru/archives/","year":"0001"},{"content":"","id":5,"section":"","summary":"search","tags":null,"title":"Search","uri":"https://ludentes.ru/search/","year":"0001"}]